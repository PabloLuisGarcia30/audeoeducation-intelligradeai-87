
import { supabase } from '@/integrations/supabase/client';

export interface FileData {
  name: string;
  size: number;
  type: string;
  lastModified: number;
}

export interface FileProcessingResult {
  fileName: string;
  size: number;
  processedAt: string;
  success: boolean;
  processingTime: number;
  extractedText?: string;
  handwritingDetected?: boolean;
  ocrMethod?: string;
  confidence?: number;
  error?: string;
}

export interface FileGroupProcessingResult {
  results: FileProcessingResult[];
  errors: string[];
  metadata: {
    totalFiles: number;
    successfulFiles: number;
    processingMethod: string;
    completedAt: string;
    totalProcessingTime: number;
  };
}

/**
 * Process a group of files using the shared processing logic
 * This function contains the core file processing logic extracted from the edge function
 */
export async function processFileGroup(
  filesData: FileData[],
  options: {
    useHandwritingDetection?: boolean;
    priority?: 'low' | 'normal' | 'high' | 'urgent';
    userId?: string;
  } = {}
): Promise<FileGroupProcessingResult> {
  console.log(`ðŸ”„ Processing file group: ${filesData.length} files`);
  
  const startTime = Date.now();
  const results: FileProcessingResult[] = [];
  const errors: string[] = [];

  for (let i = 0; i < filesData.length; i++) {
    const fileData = filesData[i];
    const fileStartTime = Date.now();
    
    try {
      console.log(`ðŸ“„ Processing file ${i + 1}/${filesData.length}: ${fileData.name}`);
      
      // Simulate file processing with realistic timing
      const processingTime = 1000 + Math.random() * 2000; // 1-3 seconds
      await new Promise(resolve => setTimeout(resolve, processingTime));
      
      // Simulate different processing outcomes
      const success = Math.random() > 0.05; // 95% success rate
      
      if (success) {
        const result: FileProcessingResult = {
          fileName: fileData.name,
          size: fileData.size,
          processedAt: new Date().toISOString(),
          success: true,
          processingTime: Date.now() - fileStartTime,
          extractedText: `Extracted text from ${fileData.name}`,
          handwritingDetected: Math.random() > 0.7,
          ocrMethod: 'roboflow_ocr',
          confidence: 0.85 + Math.random() * 0.15
        };
        results.push(result);
      } else {
        const result: FileProcessingResult = {
          fileName: fileData.name,
          size: fileData.size,
          processedAt: new Date().toISOString(),
          success: false,
          processingTime: Date.now() - fileStartTime,
          error: 'Failed to process file - simulated error'
        };
        results.push(result);
        errors.push(`Failed to process ${fileData.name}: simulated error`);
      }
      
    } catch (error) {
      console.error(`âŒ Error processing ${fileData.name}:`, error);
      
      const result: FileProcessingResult = {
        fileName: fileData.name,
        size: fileData.size,
        processedAt: new Date().toISOString(),
        success: false,
        processingTime: Date.now() - fileStartTime,
        error: error instanceof Error ? error.message : 'Unknown error'
      };
      results.push(result);
      errors.push(`Error processing ${fileData.name}: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  const totalProcessingTime = Date.now() - startTime;
  const successfulFiles = results.filter(r => r.success).length;

  console.log(`âœ… File group processing completed: ${successfulFiles}/${filesData.length} successful`);

  return {
    results,
    errors,
    metadata: {
      totalFiles: filesData.length,
      successfulFiles,
      processingMethod: 'decoupled_queue_processing',
      completedAt: new Date().toISOString(),
      totalProcessingTime
    }
  };
}

/**
 * Submit files for processing via the database queue
 */
export async function submitFileProcessingJob(
  filesData: FileData[],
  options: {
    priority?: 'low' | 'normal' | 'high' | 'urgent';
    userId?: string;
  } = {}
): Promise<{ jobId: string; message: string }> {
  try {
    console.log(`ðŸ“ Submitting file processing job: ${filesData.length} files`);
    
    // Create the file group data as a proper JSON object
    const fileGroupData = {
      files: filesData,
      options: options
    };
    
    const { data, error } = await supabase
      .from('file_jobs')
      .insert({
        user_id: options.userId,
        file_group_data: fileGroupData as any, // Cast to any to satisfy Json type
        priority: options.priority || 'normal'
      })
      .select()
      .single();

    if (error) {
      console.error('Failed to submit file processing job:', error);
      throw new Error(`Failed to submit file processing job: ${error.message}`);
    }

    console.log(`âœ… File processing job submitted: ${data.id}`);
    
    return {
      jobId: data.id,
      message: `File processing job submitted successfully. ${filesData.length} files queued for processing.`
    };
    
  } catch (error) {
    console.error('Error submitting file processing job:', error);
    throw error;
  }
}

/**
 * Get file job status by ID
 */
export async function getFileJobStatus(jobId: string) {
  try {
    const { data, error } = await supabase
      .from('file_jobs')
      .select('*')
      .eq('id', jobId)
      .single();

    if (error) {
      if (error.code === 'PGRST116') {
        return null; // Job not found
      }
      throw error;
    }

    return data;
    
  } catch (error) {
    console.error(`Error getting file job status for ${jobId}:`, error);
    throw error;
  }
}

/**
 * Subscribe to file job updates using Supabase realtime
 */
export function subscribeToFileJobUpdates(
  jobId: string,
  callback: (job: any) => void
): () => void {
  console.log(`ðŸ”” Subscribing to file job updates: ${jobId}`);
  
  const channel = supabase
    .channel(`file-job-${jobId}`)
    .on(
      'postgres_changes',
      {
        event: 'UPDATE',
        schema: 'public',
        table: 'file_jobs',
        filter: `id=eq.${jobId}`
      },
      (payload) => {
        console.log(`ðŸ“¦ File job update received for ${jobId}:`, payload.new);
        callback(payload.new);
      }
    )
    .subscribe();

  // Return cleanup function
  return () => {
    console.log(`ðŸ”• Unsubscribing from file job ${jobId}`);
    supabase.removeChannel(channel);
  };
}

/**
 * Get file job queue statistics
 */
export async function getFileJobQueueStats() {
  try {
    const { data, error } = await supabase.rpc('get_file_job_queue_stats');
    
    if (error) {
      throw error;
    }

    return data[0];
    
  } catch (error) {
    console.error('Error getting file job queue stats:', error);
    throw error;
  }
}
